ğŸ“˜ LLM Safety Category Classifier (Mini Safety Model)
---

A lightweight, fast, and fully reproducible project that classifies text into safety categories such as hate, violence, fraud, sexual content, self-harm, and benign.
This project uses OpenAI embeddings + a simple ML classifier to build a small but effective safety filter.

---

ğŸš€ 1. Problem Statement

Modern AI applications must detect unsafe or harmful content before generating responses.
Large safety models are powerful but often expensive and slow to experiment with.

Goal:
Build a small, fast, efficient safety classifier that categorizes text into:
- hate
- violence
- fraud
- sexual_content
- self_harm
- benign


This model is ideal for:
- Prototyping safety filters
- Research & learning
- Demonstrating end-to-end ML pipeline skills
- Resume/GitHub portfolio projects
- Fast on-device or API-side content moderation

---

ğŸ§© 2. Key Features
âœ” Synthetic dataset created using GPT
âœ” Embeddings generated using OpenAI text-embedding-3-large
âœ” Simple classifier (Logistic Regression / SVM / XGBoost)
âœ” Clear evaluation: accuracy, F1-score, confusion matrix
âœ” Optional: Streamlit mini dashboard
âœ” Minimal dependencies, no GPUs required
âœ” End-to-end training notebook

---

ğŸ— 3. Project Structure
ğŸ“¦ mini-safety-classifier
â”‚

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ synthetic_data.jsonl        # generated dataset
â”‚   â”œâ”€â”€ safety_embeddings.pkl       # precomputed embeddings

â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_generate_data.ipynb      # synthetic dataset creation
â”‚   â”œâ”€â”€ 02_train_classifier.ipynb   # embedding + training + evaluation
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                      # optional Streamlit mini UI
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ safety_model.pkl            # trained classifier
â”‚
â”œâ”€â”€ README.md                       # project documentation
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ .env.example                    # example for OpenAI API key
